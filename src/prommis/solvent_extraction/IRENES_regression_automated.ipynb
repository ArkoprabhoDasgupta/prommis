{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34f409e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from math import floor\n",
    "from idaes.core.util import model_statistics as mstat\n",
    "import pyomo.environ as pyo\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d6534f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocessing(df, element_list):\n",
    "\n",
    "    # Remove Nans\n",
    "    df = df.dropna()\n",
    "    df.reset_index(drop=True)\n",
    "\n",
    "    # # Remove outliers based on previewing data\n",
    "    # for s in outlier_list:\n",
    "    #     df = df.drop(index=s - 2)\n",
    "\n",
    "    # df = df.reset_index(drop=True)\n",
    "\n",
    "    # add mmols column\n",
    "    for e in element_list:\n",
    "        for i in df.index.to_list():\n",
    "            df[f\"[{e}] (mmol/L)\"] = df[f\"[{e}] (mol/L)\"] * 1000\n",
    "\n",
    "    # normalizing and creating sf dictionary\n",
    "    sf_dict = {}\n",
    "    scaled_cols = {}\n",
    "    for col in df.columns:\n",
    "        scaled_column_name = f\"{col}_scaled\"\n",
    "        if df[col].dtype in [\"float64\", \"int64\"]:\n",
    "            if col in [f\"[{e}] (mmol/L)\" for e in element_list]:\n",
    "                max_value = abs(df[col]).max()\n",
    "                scaled_cols[scaled_column_name] = df[col] / max_value\n",
    "            else:\n",
    "                max_value = 1\n",
    "                scaled_cols[scaled_column_name] = df[col] / max_value\n",
    "\n",
    "            sf_dict[col] = max_value\n",
    "            # print(f\"Scaled {col} by {max_value}\")\n",
    "    df = pd.concat([df, pd.DataFrame(scaled_cols)], axis=1)\n",
    "\n",
    "    # get scaling factors\n",
    "\n",
    "    scaling_factor = {}\n",
    "    for e in element_list:\n",
    "        scaling_factor[e] = sf_dict[f\"[{e}] (mmol/L)\"]\n",
    "\n",
    "    return df, sf_dict, scaling_factor\n",
    "\n",
    "\n",
    "# df, sf_dict, scaling_factor = data_preprocessing(df, [3, 11, 23, 25, 29, 31], Elements, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e87d079",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def regression_model_development(df, element_list, scaling_factor):\n",
    "\n",
    "    n_data = df.shape[0]\n",
    "\n",
    "    m = pyo.ConcreteModel()\n",
    "\n",
    "    m.I = pyo.RangeSet(0, n_data - 1)\n",
    "    m.element_set = pyo.Set(initialize=element_list)\n",
    "    m.f_param_index = pyo.Set(initialize=element_list + [\"constant\"])\n",
    "\n",
    "    # Input variables\n",
    "\n",
    "    m.pH = pyo.Var(m.I, domain=pyo.Reals, initialize=0.0)\n",
    "    m.C_ex = pyo.Var(m.I, domain=pyo.Reals, initialize=0.0)\n",
    "    m.Cfeed = pyo.Var(m.I, m.element_set, domain=pyo.Reals, initialize=0.0)\n",
    "\n",
    "    # Output variable\n",
    "\n",
    "    m.logD = pyo.Var(m.I, m.element_set, domain=pyo.Reals, initialize=0.0)\n",
    "\n",
    "    # Model variables\n",
    "\n",
    "    m.logD_scaled_pred = pyo.Var(m.I, m.element_set, initialize=0.1, domain=pyo.Reals)\n",
    "    m.Cext_net_pred = pyo.Var(m.I, domain=pyo.Reals, initialize=0.1, bounds=(1e-5, 2))\n",
    "    m.logCext_net_pred = pyo.Var(\n",
    "        m.I, domain=pyo.Reals, initialize=0.1, bounds=(np.log10(1e-10), np.log10(2))\n",
    "    )\n",
    "\n",
    "    # Parameters\n",
    "\n",
    "    ai_list = pyo.RangeSet(2)\n",
    "\n",
    "    m.ai = pyo.Var(ai_list, m.element_set, domain=pyo.Reals, initialize=0.5)\n",
    "    m.bi = pyo.Var(ai_list, m.element_set, domain=pyo.Reals, initialize=0.5)\n",
    "    m.ci = pyo.Var(m.element_set, m.element_set, domain=pyo.Reals, initialize=0.5)\n",
    "    m.di = pyo.Var(m.element_set, m.element_set, domain=pyo.Reals, initialize=0.0)\n",
    "    m.fi = pyo.Var(m.f_param_index, domain=pyo.NonNegativeReals, initialize=0.5)\n",
    "\n",
    "    m.alpha = pyo.Var(m.I, m.element_set, domain=pyo.Reals, initialize=0.5)\n",
    "    m.beta = pyo.Var(m.I, m.element_set, domain=pyo.Reals, initialize=0.5)\n",
    "\n",
    "    m.ai_abs = pyo.Var(\n",
    "        ai_list, m.element_set, domain=pyo.NonNegativeReals, initialize=0.5\n",
    "    )\n",
    "    m.bi_abs = pyo.Var(\n",
    "        ai_list, m.element_set, domain=pyo.NonNegativeReals, initialize=0.5\n",
    "    )\n",
    "    m.ci_abs = pyo.Var(\n",
    "        m.element_set, m.element_set, domain=pyo.NonNegativeReals, initialize=0.5\n",
    "    )\n",
    "    m.di_abs = pyo.Var(\n",
    "        m.element_set, m.element_set, domain=pyo.NonNegativeReals, initialize=0.0\n",
    "    )\n",
    "    m.fi_abs = pyo.Var(m.f_param_index, domain=pyo.NonNegativeReals, initialize=0.5)\n",
    "\n",
    "    # Initialize input variables\n",
    "\n",
    "    name_mapper_general = {\n",
    "        \"pH_scaled\": m.pH,\n",
    "        \"[D2EHPA]_scaled\": m.C_ex,\n",
    "    }\n",
    "\n",
    "    def map_generator(element):\n",
    "        return {\n",
    "            f\"[{element}] (mmol/L)_scaled\": m.Cfeed,\n",
    "            f\"log D (obs)_{element}_scaled\": m.logD,\n",
    "        }\n",
    "\n",
    "    name_mapper_elements = {}\n",
    "\n",
    "    for element in m.element_set:\n",
    "        name_mapper_elements[element] = map_generator(element)\n",
    "\n",
    "    for col in name_mapper_general.keys():\n",
    "        for i in df.index:\n",
    "            name_mapper_general[col][i].fix(df[col][i])\n",
    "\n",
    "    for element in m.element_set:\n",
    "        for i in df.index:\n",
    "            for col in name_mapper_elements[element].keys():\n",
    "                name_mapper_elements[element][col][i, element].fix(df[col][i])\n",
    "\n",
    "    # Data Fixing\n",
    "\n",
    "    def unique_element_counter(reference_element, contribution_element, df):\n",
    "        reference_element_set = df[df[f\"weight {reference_element}_scaled\"] == 1]\n",
    "        contribution_wrt_reference_set = reference_element_set[\n",
    "            reference_element_set[f\"[{contribution_element}] (mol/L)_scaled\"] != 0\n",
    "        ]\n",
    "        unique_values = len(\n",
    "            contribution_wrt_reference_set[\n",
    "                f\"[{contribution_element}] (mol/L)_scaled\"\n",
    "            ].unique()\n",
    "        )\n",
    "        return unique_values\n",
    "\n",
    "    FV = pd.DataFrame(index=m.element_set, columns=m.element_set)\n",
    "    MP = pd.DataFrame(index=m.element_set, columns=m.element_set)\n",
    "    LC = pd.DataFrame(index=m.element_set, columns=m.element_set)\n",
    "    QC = pd.DataFrame(index=m.element_set, columns=m.element_set)\n",
    "\n",
    "    for e in m.element_set:\n",
    "        for f in m.element_set:\n",
    "            FV.loc[e, f] = unique_element_counter(e, f, df)\n",
    "\n",
    "    for e in m.element_set:\n",
    "        for f in m.element_set:\n",
    "            MP.loc[e, f] = min(floor(FV.loc[e, f] / 2), 2)\n",
    "\n",
    "    for e in m.element_set:\n",
    "        for f in m.element_set:\n",
    "            LC.loc[e, f] = min(MP.loc[e, f], 1)\n",
    "\n",
    "    for e in m.element_set:\n",
    "        for f in m.element_set:\n",
    "            QC.loc[e, f] = MP.loc[e, f] - LC.loc[e, f]\n",
    "\n",
    "    for e in m.element_set:\n",
    "        for f in m.element_set:\n",
    "            if LC.loc[e, f] == 0:\n",
    "                m.ci[f, e].fix(0)\n",
    "            if QC.loc[e, f] == 0:\n",
    "                m.di[f, e].fix(0)\n",
    "\n",
    "    for e in m.element_set:\n",
    "        if (\n",
    "            len(df[df[f\"[{e}] (mol/L)_scaled\"] != 0][f\"[{e}] (mol/L)_scaled\"].unique())\n",
    "            == 1\n",
    "        ):\n",
    "            m.fi[e].fix(0)\n",
    "\n",
    "    # fixing for terms after L1 regularization analysis\n",
    "\n",
    "    for e in [\"Zn\", \"Ca\"]:\n",
    "        m.ai[1, e].fix(1e-8)\n",
    "\n",
    "    for e in [\"Mn\", \"Fe(2)\", \"Cd\", \"Mg\", \"Al\"]:\n",
    "        m.ai[2, e].fix(1e-8)\n",
    "\n",
    "    for e in [\"Fe(2)\", \"Cd\"]:\n",
    "        m.bi[1, e].fix(1e-8)\n",
    "\n",
    "    for e in [\"Mn\", \"Ni\", \"Cu\"]:\n",
    "        m.ci[\"Co\", e].fix(1e-12)\n",
    "\n",
    "    for e in [\"Ni\", \"Cu\"]:\n",
    "        m.ci[e, \"Co\"].fix(1e-12)\n",
    "\n",
    "    for e in [\"Mn\", \"Cu\"]:\n",
    "        m.di[\"Co\", e].fix(1e-12)\n",
    "\n",
    "    for e in [\"Mn\", \"Co\"]:\n",
    "        m.di[\"Ni\", e].fix(1e-12)\n",
    "\n",
    "    for e in [\"Mn\", \"Co\", \"Ni\"]:\n",
    "        m.di[\"Cu\", e].fix(1e-12)\n",
    "\n",
    "    # Second reduction\n",
    "\n",
    "    for e in [\"Mn\", \"Co\", \"Fe(3)\"]:\n",
    "        m.fi[e].fix(1e-14)\n",
    "\n",
    "    for e in [\"Mn\", \"Ni\"]:\n",
    "        m.di[\"Mn\", e].fix(1e-12)\n",
    "\n",
    "    # Constraints\n",
    "\n",
    "    @m.Constraint(m.I)\n",
    "    def logCext_net_pred_constraint(m, i):\n",
    "        return 10 ** (m.logCext_net_pred[i]) == m.Cext_net_pred[i]\n",
    "\n",
    "    @m.Constraint(m.I, m.element_set)\n",
    "    def alpha_const(m, i, e):\n",
    "        linear_term = sum(m.ci[s, e] * m.Cfeed[i, s] for s in m.element_set)\n",
    "        quad_term = sum(m.di[s, e] * m.Cfeed[i, s] ** 2 for s in m.element_set)\n",
    "\n",
    "        return (\n",
    "            m.alpha[i, e]\n",
    "            - m.ai[1, e]\n",
    "            - m.ai[2, e] * m.C_ex[i]\n",
    "            - linear_term\n",
    "            - quad_term\n",
    "        ) == 0\n",
    "\n",
    "    @m.Constraint(m.I, m.element_set)\n",
    "    def b_const(m, i, e):\n",
    "        return m.beta[i, e] == m.bi[1, e] + m.bi[2, e] * m.logCext_net_pred[i]\n",
    "\n",
    "    @m.Constraint(m.I)\n",
    "    def Cext_net_pred_constraint(m, i):\n",
    "        linear_term = sum(m.fi[s] * m.Cfeed[i, s] for s in m.element_set)\n",
    "        return m.Cext_net_pred[i] == m.C_ex[i] - linear_term - m.fi[\"constant\"]\n",
    "\n",
    "    @m.Constraint(m.I)\n",
    "    def Cext_net_value_constraint(m, i):\n",
    "        return m.C_ex[i] - m.Cext_net_pred[i] >= 1e-8\n",
    "\n",
    "    @m.Constraint(m.I, m.element_set)\n",
    "    def logD_scaled_pred_constraint(m, i, e):\n",
    "        return m.logD_scaled_pred[i, e] == m.alpha[i, e] * m.pH[i] + m.beta[i, e]\n",
    "\n",
    "    @m.Constraint(ai_list, m.element_set)\n",
    "    def ai_negative_bound(m, i, e):\n",
    "        if m.ai[i, e].fixed == True:\n",
    "            m.ai_abs[i, e].fix(1e-9)\n",
    "            return pyo.Constraint.Skip\n",
    "        else:\n",
    "            return m.ai_abs[i, e] >= -m.ai[i, e]\n",
    "\n",
    "    @m.Constraint(ai_list, m.element_set)\n",
    "    def ai_positive_bound(m, i, e):\n",
    "        if m.ai[i, e].fixed == True:\n",
    "            m.ai_abs[i, e].fix(1e-9)\n",
    "            return pyo.Constraint.Skip\n",
    "        else:\n",
    "            return m.ai_abs[i, e] >= m.ai[i, e]\n",
    "\n",
    "    @m.Constraint(ai_list, m.element_set)\n",
    "    def bi_negative_bound(m, i, e):\n",
    "        if m.bi[i, e].fixed == True:\n",
    "            m.bi_abs[i, e].fix(1e-9)\n",
    "            return pyo.Constraint.Skip\n",
    "        else:\n",
    "            return m.bi_abs[i, e] >= -m.bi[i, e]\n",
    "\n",
    "    @m.Constraint(ai_list, m.element_set)\n",
    "    def bi_positive_bound(m, i, e):\n",
    "        if m.bi[i, e].fixed == True:\n",
    "            m.bi_abs[i, e].fix(1e-9)\n",
    "            return pyo.Constraint.Skip\n",
    "        else:\n",
    "            return m.bi_abs[i, e] >= m.bi[i, e]\n",
    "\n",
    "    @m.Constraint(m.element_set, m.element_set)\n",
    "    def ci_negative_bound(m, f, e):\n",
    "        if m.ci[f, e].fixed == True:\n",
    "            m.ci_abs[f, e].fix(1e-9)\n",
    "            return pyo.Constraint.Skip\n",
    "        else:\n",
    "            return m.ci_abs[f, e] >= -m.ci[f, e]\n",
    "\n",
    "    @m.Constraint(m.element_set, m.element_set)\n",
    "    def ci_positive_bound(m, f, e):\n",
    "        if m.ci[f, e].fixed == True:\n",
    "            m.ci_abs[f, e].fix(1e-9)\n",
    "            return pyo.Constraint.Skip\n",
    "        else:\n",
    "            return m.ci_abs[f, e] >= m.ci[f, e]\n",
    "\n",
    "    @m.Constraint(m.element_set, m.element_set)\n",
    "    def di_negative_bound(m, f, e):\n",
    "        if m.di[f, e].fixed == True:\n",
    "            m.di_abs[f, e].fix(1e-9)\n",
    "            return pyo.Constraint.Skip\n",
    "        else:\n",
    "            return m.di_abs[f, e] >= -m.di[f, e]\n",
    "\n",
    "    @m.Constraint(m.element_set, m.element_set)\n",
    "    def di_positive_bound(m, f, e):\n",
    "        if m.di[f, e].fixed == True:\n",
    "            m.di_abs[f, e].fix(1e-9)\n",
    "            return pyo.Constraint.Skip\n",
    "        else:\n",
    "            return m.di_abs[f, e] >= m.di[f, e]\n",
    "\n",
    "    @m.Constraint(m.f_param_index)\n",
    "    def fi_negative_bound(m, f):\n",
    "        if m.fi[f].fixed == True:\n",
    "            m.fi_abs[f].fix(1e-9)\n",
    "            return pyo.Constraint.Skip\n",
    "        else:\n",
    "            return m.fi_abs[f] >= -m.fi[f]\n",
    "\n",
    "    @m.Constraint(m.f_param_index)\n",
    "    def fi_positive_bound(m, f):\n",
    "        if m.fi[f].fixed == True:\n",
    "            m.fi_abs[f].fix(1e-9)\n",
    "            return pyo.Constraint.Skip\n",
    "        else:\n",
    "            return m.fi_abs[f] >= m.fi[f]\n",
    "\n",
    "    @m.Expression()\n",
    "    def sse_element(m):\n",
    "        n_data = len(m.I)\n",
    "        n_elements = len(m.element_set)\n",
    "        return sum(\n",
    "            ((m.logD_scaled_pred[i, e] - m.logD[i, e]) ** 2)\n",
    "            * df[f\"weight {e}_scaled\"][i]\n",
    "            for i in m.I\n",
    "            for e in m.element_set\n",
    "        ) / (n_data * n_elements)\n",
    "\n",
    "    @m.Expression()\n",
    "    def l1_norm_term(m):\n",
    "        return (\n",
    "            sum(\n",
    "                (m.ai_abs[1, e])\n",
    "                + (m.ai_abs[2, e])\n",
    "                + (m.bi_abs[1, e])\n",
    "                + (m.bi_abs[2, e])\n",
    "                + sum((m.ci_abs[s, e]) / scaling_factor[s] for s in m.element_set)\n",
    "                + sum((m.di_abs[s, e]) / scaling_factor[s] ** 2 for s in m.element_set)\n",
    "                + (m.fi_abs[e])\n",
    "                for e in m.element_set\n",
    "            )\n",
    "            + m.fi_abs[\"constant\"]\n",
    "        )\n",
    "\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75ac045e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_buildup_function(m, rho):\n",
    "\n",
    "    m.rho = pyo.Param(initialize=rho, mutable=True)\n",
    "\n",
    "    @m.Objective(sense=pyo.minimize)\n",
    "    def objective_function(m):\n",
    "        return m.sse_element + m.rho * m.l1_norm_term\n",
    "\n",
    "\n",
    "def model_solution(m):\n",
    "\n",
    "    solver = pyo.SolverFactory(\"ipopt_v2\")\n",
    "    solver.options[\"halt_on_ampl_error\"] = \"no\"\n",
    "    solver.options[\"max_iter\"] = 500\n",
    "\n",
    "    print(mstat.degrees_of_freedom(m))\n",
    "\n",
    "    solver.solve(m, tee=True, symbolic_solver_labels=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c93fc103",
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_regression_analysis(\n",
    "    df_raw, outlier_list, element_list, conversion_factor, rho\n",
    "):\n",
    "    df, sf_dict, scaling_factor = data_preprocessing(\n",
    "        df_raw, outlier_list, element_list, conversion_factor\n",
    "    )\n",
    "    m = regression_model_development(df, element_list, scaling_factor)\n",
    "    objective_buildup_function(m, rho)\n",
    "    m, results = model_solution(m)\n",
    "    return m, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e8f870c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "\n",
    "def R_squared_calculation(m, df):\n",
    "    r_square_value = {}\n",
    "\n",
    "    for e in m.element_set:\n",
    "        logD_scaled_pred_eff = [\n",
    "            m.logD_scaled_pred[i, e]() * df[f\"weight {e}_scaled\"][i] for i in m.I\n",
    "        ]\n",
    "        logD_scaled_eff = [m.logD[i, e]() * df[f\"weight {e}_scaled\"][i] for i in m.I]\n",
    "        r_square_value[e] = r2_score(logD_scaled_eff, logD_scaled_pred_eff)\n",
    "\n",
    "    return r_square_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "567f0049",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_path = \"General_dataset_DEHPA.xlsx\"\n",
    "\n",
    "df1 = pd.read_excel(data_path, sheet_name=\"DaSilva\")\n",
    "df2 = pd.read_excel(data_path, sheet_name=\"Syensqo\")\n",
    "df3 = pd.read_excel(data_path, sheet_name=\"Peng\")\n",
    "df4 = pd.read_excel(data_path, sheet_name=\"Metallurgist\")\n",
    "\n",
    "df_total = pd.concat([df1, df2, df3, df4], ignore_index=True)\n",
    "\n",
    "Elements = [\n",
    "    \"Mn\",\n",
    "    \"Co\",\n",
    "    \"Ni\",\n",
    "    \"Cu\",\n",
    "    \"Zn\",\n",
    "    \"Fe(3)\",\n",
    "    \"Ca\",\n",
    "    \"Fe(2)\",\n",
    "    \"Cd\",\n",
    "    \"Mg\",\n",
    "    \"Al\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a3e98ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ad00105\\AppData\\Local\\Temp\\1\\ipykernel_21384\\10811894.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[f\"[{e}] (mmol/L)\"] = df[f\"[{e}] (mol/L)\"] * 1000\n",
      "C:\\Users\\ad00105\\AppData\\Local\\Temp\\1\\ipykernel_21384\\10811894.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[f\"[{e}] (mmol/L)\"] = df[f\"[{e}] (mol/L)\"] * 1000\n",
      "C:\\Users\\ad00105\\AppData\\Local\\Temp\\1\\ipykernel_21384\\10811894.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[f\"[{e}] (mmol/L)\"] = df[f\"[{e}] (mol/L)\"] * 1000\n",
      "C:\\Users\\ad00105\\AppData\\Local\\Temp\\1\\ipykernel_21384\\10811894.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[f\"[{e}] (mmol/L)\"] = df[f\"[{e}] (mol/L)\"] * 1000\n",
      "C:\\Users\\ad00105\\AppData\\Local\\Temp\\1\\ipykernel_21384\\10811894.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[f\"[{e}] (mmol/L)\"] = df[f\"[{e}] (mol/L)\"] * 1000\n",
      "C:\\Users\\ad00105\\AppData\\Local\\Temp\\1\\ipykernel_21384\\10811894.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[f\"[{e}] (mmol/L)\"] = df[f\"[{e}] (mol/L)\"] * 1000\n",
      "C:\\Users\\ad00105\\AppData\\Local\\Temp\\1\\ipykernel_21384\\10811894.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[f\"[{e}] (mmol/L)\"] = df[f\"[{e}] (mol/L)\"] * 1000\n",
      "C:\\Users\\ad00105\\AppData\\Local\\Temp\\1\\ipykernel_21384\\10811894.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[f\"[{e}] (mmol/L)\"] = df[f\"[{e}] (mol/L)\"] * 1000\n",
      "C:\\Users\\ad00105\\AppData\\Local\\Temp\\1\\ipykernel_21384\\10811894.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[f\"[{e}] (mmol/L)\"] = df[f\"[{e}] (mol/L)\"] * 1000\n",
      "C:\\Users\\ad00105\\AppData\\Local\\Temp\\1\\ipykernel_21384\\10811894.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[f\"[{e}] (mmol/L)\"] = df[f\"[{e}] (mol/L)\"] * 1000\n",
      "C:\\Users\\ad00105\\AppData\\Local\\Temp\\1\\ipykernel_21384\\10811894.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[f\"[{e}] (mmol/L)\"] = df[f\"[{e}] (mol/L)\"] * 1000\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"Index '162' is not valid for indexed component 'pH'\"",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      6\u001b[39m threshold = \u001b[32m0.83\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Make the model\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m m = \u001b[43mregression_model_development\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mElements\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaling_factor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Make the objective function\u001b[39;00m\n\u001b[32m     12\u001b[39m rho = \u001b[32m0.0312\u001b[39m / \u001b[32m186\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 76\u001b[39m, in \u001b[36mregression_model_development\u001b[39m\u001b[34m(df, element_list, scaling_factor)\u001b[39m\n\u001b[32m     74\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m name_mapper_general.keys():\n\u001b[32m     75\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m df.index:\n\u001b[32m---> \u001b[39m\u001b[32m76\u001b[39m         \u001b[43mname_mapper_general\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m.fix(df[col][i])\n\u001b[32m     78\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m element \u001b[38;5;129;01min\u001b[39;00m m.element_set:\n\u001b[32m     79\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m df.index:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ad00105\\Anaconda3\\envs\\prommis-dev\\Lib\\site-packages\\pyomo\\core\\base\\var.py:999\u001b[39m, in \u001b[36mIndexedVar.__getitem__\u001b[39m\u001b[34m(self, args)\u001b[39m\n\u001b[32m    997\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, args) -> VarData:\n\u001b[32m    998\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m999\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__getitem__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1000\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m:\n\u001b[32m   1001\u001b[39m         tmp = args \u001b[38;5;28;01mif\u001b[39;00m args.\u001b[34m__class__\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m (args,)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ad00105\\Anaconda3\\envs\\prommis-dev\\Lib\\site-packages\\pyomo\\core\\base\\indexed_component.py:648\u001b[39m, in \u001b[36mIndexedComponent.__getitem__\u001b[39m\u001b[34m(self, index)\u001b[39m\n\u001b[32m    646\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(index, EXPR.GetItemExpression):\n\u001b[32m    647\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m index\n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m validated_index = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m validated_index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m index:\n\u001b[32m    650\u001b[39m     index = validated_index\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ad00105\\Anaconda3\\envs\\prommis-dev\\Lib\\site-packages\\pyomo\\core\\base\\indexed_component.py:870\u001b[39m, in \u001b[36mIndexedComponent._validate_index\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m    863\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\n\u001b[32m    864\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCannot treat the scalar component \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    865\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mas an indexed component\u001b[39m\u001b[33m\"\u001b[39m % (\u001b[38;5;28mself\u001b[39m.name,)\n\u001b[32m    866\u001b[39m     )\n\u001b[32m    867\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    868\u001b[39m \u001b[38;5;66;03m# Raise an exception\u001b[39;00m\n\u001b[32m    869\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m870\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\n\u001b[32m    871\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mIndex \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m is not valid for indexed component \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    872\u001b[39m     % (normalized_idx, \u001b[38;5;28mself\u001b[39m.name)\n\u001b[32m    873\u001b[39m )\n",
      "\u001b[31mKeyError\u001b[39m: \"Index '162' is not valid for indexed component 'pH'\""
     ]
    }
   ],
   "source": [
    "# Preprocess the data\n",
    "df, sf_dict, scaling_factor = data_preprocessing(\n",
    "    df_total, Elements\n",
    ")\n",
    "\n",
    "threshold = 0.83\n",
    "\n",
    "# Make the model\n",
    "m = regression_model_development(df, Elements, scaling_factor)\n",
    "\n",
    "# Make the objective function\n",
    "rho = 0.0312 / 186\n",
    "objective_buildup_function(m, rho)\n",
    "\n",
    "# Solve the model\n",
    "model_solution(m)\n",
    "\n",
    "# Get the R squared values\n",
    "r_square_old = R_squared_calculation(m, df)\n",
    "\n",
    "# Get the elements with lower R squared values and sort them\n",
    "lower_rsquare_elements = {\n",
    "    k: r_square_old[k] for k in r_square_old.keys() if r_square_old[k] < threshold\n",
    "}\n",
    "\n",
    "# go into loop\n",
    "\n",
    "# if lower_rsquare_elements is empty, stop the regression analysis\n",
    "while lower_rsquare_elements != {}:\n",
    "\n",
    "    # get element having least R square value\n",
    "    element_having_least_R = [\n",
    "        k\n",
    "        for k in lower_rsquare_elements.keys()\n",
    "        if lower_rsquare_elements[k] == min(v for v in lower_rsquare_elements.values())\n",
    "    ]\n",
    "    print(f'element with least R is {element_having_least_R}')\n",
    "\n",
    "    # remove that data point from database and reperform regression\n",
    "    print('resolving after removing element')\n",
    "    df.loc[element_having_least_R, f\"weight {element_having_least_R}_scaled\"] = 0\n",
    "    model_solution(m)\n",
    "\n",
    "    # get the new R square values\n",
    "    r_square_new = R_squared_calculation(m, df)\n",
    "\n",
    "    # Compare the R squared values\n",
    "    if (\n",
    "        r_square_new[element_having_least_R] - r_square_old[element_having_least_R]\n",
    "    ) < 0.03:\n",
    "        print(f'no significant improvement in R value for {element_having_least_R}')\n",
    "        df.loc[element_having_least_R, f\"weight {element_having_least_R}_scaled\"] = 1\n",
    "        print(f\"this is the best model possible for {element_having_least_R}\")\n",
    "        del lower_rsquare_elements[element_having_least_R]\n",
    "    else:\n",
    "        print(f'significant improvement in R value for {element_having_least_R}, continuing regression')\n",
    "        r_square_old = r_square_new\n",
    "        lower_rsquare_elements = {\n",
    "            k: r_square_old[k] for k in r_square_old.keys() if r_square_old[k] < threshold\n",
    "        }\n",
    "    \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prommis-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
